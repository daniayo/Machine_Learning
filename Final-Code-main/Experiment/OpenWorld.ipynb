{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWWwWvfkZQy1"
   },
   "source": [
    "## Open world : Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MklnTMfZUIW",
    "outputId": "5460470b-2a42-4b56-e8d9-73774a960aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total monitored samples: 19000\n",
      "Loading unmonitored datafile...\n",
      "Total combined samples: 22000\n",
      "Training Open-World Binary Decision Tree model...\n",
      "Binary Classification Accuracy (Decision Tree): 0.9998\n",
      "\n",
      "Binary Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       590\n",
      "           1       1.00      1.00      1.00      3810\n",
      "\n",
      "    accuracy                           1.00      4400\n",
      "   macro avg       1.00      1.00      1.00      4400\n",
      "weighted avg       1.00      1.00      1.00      4400\n",
      "\n",
      "Training Open-World Multi-Class Decision Tree model...\n",
      "Multi-Class Classification Accuracy (Decision Tree): 0.8473\n",
      "\n",
      "Multi-Class Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       590\n",
      "           0       0.84      0.70      0.76        30\n",
      "           1       0.91      0.89      0.90        44\n",
      "           2       0.83      0.83      0.83        41\n",
      "           3       0.90      0.85      0.88        33\n",
      "           4       0.76      0.81      0.79        32\n",
      "           5       0.82      0.89      0.86        37\n",
      "           6       0.84      0.95      0.89        38\n",
      "           7       0.72      0.74      0.73        35\n",
      "           8       0.73      0.73      0.73        33\n",
      "           9       0.41      0.62      0.49        26\n",
      "          10       0.92      0.79      0.85        43\n",
      "          11       0.85      0.87      0.86        45\n",
      "          12       0.97      0.84      0.90        44\n",
      "          13       0.67      0.70      0.68        46\n",
      "          14       0.88      1.00      0.94        37\n",
      "          15       0.73      0.77      0.75        39\n",
      "          16       0.84      0.73      0.78        51\n",
      "          17       0.85      0.74      0.79        53\n",
      "          18       0.88      1.00      0.94        38\n",
      "          19       0.70      0.82      0.76        38\n",
      "          20       0.96      0.96      0.96        48\n",
      "          21       0.97      1.00      0.99        34\n",
      "          22       0.76      0.61      0.67        51\n",
      "          23       0.78      0.79      0.78        39\n",
      "          24       0.59      0.53      0.56        38\n",
      "          25       0.82      0.74      0.78        38\n",
      "          26       0.94      0.84      0.89        37\n",
      "          27       0.89      0.89      0.89        47\n",
      "          28       0.67      0.84      0.74        31\n",
      "          29       0.93      0.86      0.89        43\n",
      "          30       0.81      0.90      0.85        39\n",
      "          31       0.84      0.82      0.83        44\n",
      "          32       0.69      0.71      0.70        48\n",
      "          33       0.95      0.90      0.92        39\n",
      "          34       0.76      0.69      0.72        36\n",
      "          35       0.82      0.84      0.83        32\n",
      "          36       0.86      0.88      0.87        42\n",
      "          37       0.68      0.71      0.70        42\n",
      "          38       0.63      0.59      0.61        49\n",
      "          39       0.86      0.72      0.78        43\n",
      "          40       0.69      0.76      0.72        45\n",
      "          41       0.81      0.83      0.82        46\n",
      "          42       0.90      0.88      0.89        43\n",
      "          43       0.89      0.84      0.86        49\n",
      "          44       0.89      0.98      0.93        42\n",
      "          45       0.68      0.67      0.67        45\n",
      "          46       0.93      0.87      0.90        31\n",
      "          47       0.89      0.81      0.85        48\n",
      "          48       0.98      0.93      0.95        44\n",
      "          49       0.88      0.82      0.85        34\n",
      "          50       0.88      0.91      0.89        32\n",
      "          51       0.98      0.94      0.96        48\n",
      "          52       0.89      0.72      0.80        57\n",
      "          53       0.80      0.88      0.84        41\n",
      "          54       0.71      0.89      0.79        36\n",
      "          55       0.86      0.84      0.85        44\n",
      "          56       0.97      0.95      0.96        41\n",
      "          57       0.97      0.91      0.94        32\n",
      "          58       0.90      0.90      0.90        42\n",
      "          59       0.94      0.89      0.91        36\n",
      "          60       0.63      0.77      0.69        35\n",
      "          61       0.76      0.78      0.77        37\n",
      "          62       0.77      0.79      0.78        43\n",
      "          63       0.88      0.86      0.87        43\n",
      "          64       0.84      0.89      0.86        35\n",
      "          65       0.74      0.82      0.78        39\n",
      "          66       0.93      0.77      0.84        35\n",
      "          67       0.76      0.90      0.83        42\n",
      "          68       0.75      0.82      0.79        40\n",
      "          69       0.90      0.90      0.90        42\n",
      "          70       0.91      1.00      0.95        41\n",
      "          71       0.81      0.90      0.85        29\n",
      "          72       0.89      0.89      0.89        56\n",
      "          73       0.93      0.93      0.93        43\n",
      "          74       0.77      0.75      0.76        32\n",
      "          75       0.89      1.00      0.94        40\n",
      "          76       0.85      0.92      0.88        37\n",
      "          77       0.71      0.71      0.71        31\n",
      "          78       0.93      0.82      0.87        33\n",
      "          79       0.68      0.75      0.71        53\n",
      "          80       1.00      0.93      0.96        40\n",
      "          81       0.94      0.74      0.83        39\n",
      "          82       0.70      0.75      0.72        40\n",
      "          83       0.75      0.69      0.72        39\n",
      "          84       0.85      0.78      0.81        36\n",
      "          85       1.00      0.85      0.92        47\n",
      "          86       0.80      0.69      0.74        35\n",
      "          87       0.76      0.86      0.81        37\n",
      "          88       0.62      0.78      0.69        27\n",
      "          89       0.74      0.70      0.72        40\n",
      "          90       0.92      0.88      0.90        51\n",
      "          91       0.84      0.80      0.82        40\n",
      "          92       0.70      0.79      0.74        29\n",
      "          93       0.94      0.92      0.93        50\n",
      "          94       0.81      0.83      0.82        35\n",
      "\n",
      "    accuracy                           0.85      4400\n",
      "   macro avg       0.83      0.83      0.82      4400\n",
      "weighted avg       0.85      0.85      0.85      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS = 950\n",
    "\n",
    "print(\"Loading data...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_timestamps = []  # Packet timestamps\n",
    "X_packet_sizes = []  # Packet sizes\n",
    "X_cum_sizes = []  # Cumulative packet size\n",
    "y_monitored = []  # Labels\n",
    "\n",
    "for i in range(TOTAL_URLS):\n",
    "    label = i if USE_SUBLABEL else i // URL_PER_SITE\n",
    "    for sample in data[i]:\n",
    "        timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "        packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "        cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "        cumulative_sum = 0\n",
    "        for j, c in enumerate(sample):\n",
    "            dr = 1 if c > 0 else -1\n",
    "            timestamps[j] = abs(c)\n",
    "            packet_sizes[j] = dr * 512\n",
    "            cumulative_sum += packet_sizes[j]\n",
    "            cum_sizes[j] = cumulative_sum\n",
    "\n",
    "        X_timestamps.append(timestamps)\n",
    "        X_packet_sizes.append(packet_sizes)\n",
    "        X_cum_sizes.append(cum_sizes)\n",
    "        y_monitored.append(label)\n",
    "\n",
    "print(f\"Total monitored samples: {len(y_monitored)}\")\n",
    "\n",
    "TOTAL_UNMON_URLS = 3000\n",
    "\n",
    "print(\"Loading unmonitored datafile...\")\n",
    "with open(\"/content/sample_data/unmon_standard10_3000.pkl\", \"rb\") as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "for i in range(TOTAL_UNMON_URLS):\n",
    "    sample = x[i]\n",
    "    timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "    packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "    cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "    cumulative_sum = 0\n",
    "    for j, c in enumerate(sample):\n",
    "        dr = 1 if c > 0 else -1\n",
    "        timestamps[j] = abs(c)\n",
    "        packet_sizes[j] = dr * 512\n",
    "        cumulative_sum += packet_sizes[j]\n",
    "        cum_sizes[j] = cumulative_sum\n",
    "\n",
    "    X_timestamps.append(timestamps)\n",
    "    X_packet_sizes.append(packet_sizes)\n",
    "    X_cum_sizes.append(cum_sizes)\n",
    "    y_monitored.append(-1)  # Label unmonitored data as -1\n",
    "\n",
    "print(f\"Total combined samples: {len(y_monitored)}\")\n",
    "\n",
    "# padding\n",
    "max_length = max(len(seq) for seq in X_timestamps)\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq[:maxlen]\n",
    "    return padded_sequences\n",
    "\n",
    "X_timestamps_padded = pad_sequences(X_timestamps, max_length)\n",
    "X_packet_sizes_padded = pad_sequences(X_packet_sizes, max_length)\n",
    "X_cum_sizes_padded = pad_sequences(X_cum_sizes, max_length)\n",
    "\n",
    "# feature combination\n",
    "X_combined = np.hstack((X_timestamps_padded, X_packet_sizes_padded, X_cum_sizes_padded))\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_monitored, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binary Classification (Monitored vs Unmonitored)\n",
    "binary_y_train = np.where(np.array(y_train) == -1, -1, 1)  # Monitored(1), Unmonitored(-1)\n",
    "binary_y_test = np.where(np.array(y_test) == -1, -1, 1)\n",
    "\n",
    "# Decision Tree Binary Classification\n",
    "print(\"Training Open-World Binary Decision Tree model...\")\n",
    "dt_binary_model = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "dt_binary_model.fit(X_train, binary_y_train)\n",
    "\n",
    "binary_y_pred_dt = dt_binary_model.predict(X_test)\n",
    "binary_accuracy_dt = accuracy_score(binary_y_test, binary_y_pred_dt)\n",
    "\n",
    "print(f\"Binary Classification Accuracy (Decision Tree): {binary_accuracy_dt:.4f}\")\n",
    "print(\"\\nBinary Classification Report (Decision Tree):\")\n",
    "print(classification_report(binary_y_test, binary_y_pred_dt))\n",
    "\n",
    "# Multi-Class Classification (Monitored and Unmonitored)\n",
    "print(\"Training Open-World Multi-Class Decision Tree model...\")\n",
    "dt_multi_model = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "dt_multi_model.fit(X_train, y_train)\n",
    "\n",
    "multi_y_pred_dt = dt_multi_model.predict(X_test)\n",
    "multi_accuracy_dt = accuracy_score(y_test, multi_y_pred_dt)\n",
    "\n",
    "print(f\"Multi-Class Classification Accuracy (Decision Tree): {multi_accuracy_dt:.4f}\")\n",
    "print(\"\\nMulti-Class Classification Report (Decision Tree):\")\n",
    "print(classification_report(y_test, multi_y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocKEEH0vZWwI"
   },
   "source": [
    "## Open world : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVVRaBtdZb4a",
    "outputId": "ed000a4e-1b53-42f9-e426-505c43f505d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading monitored datafile...\n",
      "Total monitored samples: 19000\n",
      "Loading unmonitored datafile...\n",
      "Total combined samples: 22000\n",
      "Training Open-World Binary Random Forest model...\n",
      "Binary Classification Accuracy (Random Forest): 0.9932\n",
      "\n",
      "Binary Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.95      0.97       590\n",
      "           1       0.99      1.00      1.00      3810\n",
      "\n",
      "    accuracy                           0.99      4400\n",
      "   macro avg       1.00      0.98      0.99      4400\n",
      "weighted avg       0.99      0.99      0.99      4400\n",
      "\n",
      "Training Open-World Multi-Class Random Forest model...\n",
      "Multi-Class Classification Accuracy (Random Forest): 0.8925\n",
      "\n",
      "Multi-Class Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.95      0.83       590\n",
      "           0       0.96      0.90      0.93        30\n",
      "           1       0.89      0.75      0.81        44\n",
      "           2       0.97      0.88      0.92        41\n",
      "           3       0.91      0.88      0.89        33\n",
      "           4       0.94      0.97      0.95        32\n",
      "           5       0.87      0.92      0.89        37\n",
      "           6       0.88      0.95      0.91        38\n",
      "           7       0.80      1.00      0.89        35\n",
      "           8       0.97      0.85      0.90        33\n",
      "           9       0.89      0.92      0.91        26\n",
      "          10       0.95      0.81      0.88        43\n",
      "          11       1.00      0.89      0.94        45\n",
      "          12       0.87      0.89      0.88        44\n",
      "          13       0.85      0.76      0.80        46\n",
      "          14       0.97      0.81      0.88        37\n",
      "          15       0.73      0.97      0.84        39\n",
      "          16       0.95      0.78      0.86        51\n",
      "          17       1.00      0.89      0.94        53\n",
      "          18       1.00      0.95      0.97        38\n",
      "          19       0.97      0.87      0.92        38\n",
      "          20       0.96      0.98      0.97        48\n",
      "          21       0.94      0.88      0.91        34\n",
      "          22       0.86      0.94      0.90        51\n",
      "          23       0.92      0.90      0.91        39\n",
      "          24       0.82      0.61      0.70        38\n",
      "          25       0.89      0.84      0.86        38\n",
      "          26       0.75      0.89      0.81        37\n",
      "          27       0.98      0.89      0.93        47\n",
      "          28       0.91      0.97      0.94        31\n",
      "          29       0.76      0.88      0.82        43\n",
      "          30       0.94      0.82      0.88        39\n",
      "          31       0.91      0.98      0.95        44\n",
      "          32       0.93      0.85      0.89        48\n",
      "          33       1.00      0.90      0.95        39\n",
      "          34       0.91      0.89      0.90        36\n",
      "          35       1.00      0.91      0.95        32\n",
      "          36       0.98      0.98      0.98        42\n",
      "          37       1.00      0.79      0.88        42\n",
      "          38       0.93      0.84      0.88        49\n",
      "          39       0.95      0.88      0.92        43\n",
      "          40       0.93      0.89      0.91        45\n",
      "          41       0.89      0.89      0.89        46\n",
      "          42       0.93      0.86      0.89        43\n",
      "          43       0.96      0.94      0.95        49\n",
      "          44       0.98      1.00      0.99        42\n",
      "          45       0.93      0.89      0.91        45\n",
      "          46       1.00      1.00      1.00        31\n",
      "          47       0.93      0.83      0.88        48\n",
      "          48       0.95      0.91      0.93        44\n",
      "          49       0.90      0.82      0.86        34\n",
      "          50       0.87      0.81      0.84        32\n",
      "          51       0.90      0.90      0.90        48\n",
      "          52       0.94      0.81      0.87        57\n",
      "          53       0.95      0.93      0.94        41\n",
      "          54       0.91      0.89      0.90        36\n",
      "          55       0.98      0.93      0.95        44\n",
      "          56       0.97      0.93      0.95        41\n",
      "          57       0.86      1.00      0.93        32\n",
      "          58       0.97      0.93      0.95        42\n",
      "          59       0.94      0.94      0.94        36\n",
      "          60       0.85      0.97      0.91        35\n",
      "          61       0.94      0.89      0.92        37\n",
      "          62       0.93      0.91      0.92        43\n",
      "          63       0.83      0.81      0.82        43\n",
      "          64       1.00      0.89      0.94        35\n",
      "          65       0.84      0.79      0.82        39\n",
      "          66       0.94      0.83      0.88        35\n",
      "          67       1.00      0.95      0.98        42\n",
      "          68       0.97      0.85      0.91        40\n",
      "          69       0.97      0.86      0.91        42\n",
      "          70       0.95      0.98      0.96        41\n",
      "          71       1.00      0.97      0.98        29\n",
      "          72       1.00      0.91      0.95        56\n",
      "          73       0.98      0.93      0.95        43\n",
      "          74       0.94      0.97      0.95        32\n",
      "          75       0.95      0.97      0.96        40\n",
      "          76       0.97      0.86      0.91        37\n",
      "          77       0.93      0.87      0.90        31\n",
      "          78       0.85      0.85      0.85        33\n",
      "          79       0.93      0.77      0.85        53\n",
      "          80       0.97      0.95      0.96        40\n",
      "          81       0.97      0.74      0.84        39\n",
      "          82       0.82      0.70      0.76        40\n",
      "          83       0.89      0.85      0.87        39\n",
      "          84       1.00      0.81      0.89        36\n",
      "          85       0.98      0.94      0.96        47\n",
      "          86       0.81      0.97      0.88        35\n",
      "          87       0.97      0.92      0.94        37\n",
      "          88       0.93      0.93      0.93        27\n",
      "          89       0.88      0.90      0.89        40\n",
      "          90       0.96      0.92      0.94        51\n",
      "          91       0.92      0.82      0.87        40\n",
      "          92       0.92      0.76      0.83        29\n",
      "          93       0.91      0.84      0.88        50\n",
      "          94       0.97      0.91      0.94        35\n",
      "\n",
      "    accuracy                           0.89      4400\n",
      "   macro avg       0.93      0.89      0.90      4400\n",
      "weighted avg       0.90      0.89      0.89      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS = 950\n",
    "\n",
    "print(\"Loading monitored datafile...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_timestamps = []  # Packet timestamps\n",
    "X_packet_sizes = []  # Packet sizes\n",
    "X_cum_sizes = []  # Cumulative packet size\n",
    "y_monitored = []  # Labels\n",
    "\n",
    "for i in range(TOTAL_URLS):\n",
    "    label = i if USE_SUBLABEL else i // URL_PER_SITE\n",
    "    for sample in data[i]:\n",
    "        timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "        packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "        cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "        cumulative_sum = 0\n",
    "        for j, c in enumerate(sample):\n",
    "            dr = 1 if c > 0 else -1\n",
    "            timestamps[j] = abs(c)\n",
    "            packet_sizes[j] = dr * 512\n",
    "            cumulative_sum += packet_sizes[j]\n",
    "            cum_sizes[j] = cumulative_sum\n",
    "\n",
    "        X_timestamps.append(timestamps)\n",
    "        X_packet_sizes.append(packet_sizes)\n",
    "        X_cum_sizes.append(cum_sizes)\n",
    "        y_monitored.append(label)\n",
    "\n",
    "print(f\"Total monitored samples: {len(y_monitored)}\")\n",
    "\n",
    "TOTAL_UNMON_URLS = 3000\n",
    "\n",
    "print(\"Loading unmonitored datafile...\")\n",
    "with open(\"/content/sample_data/unmon_standard10_3000.pkl\", \"rb\") as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "for i in range(TOTAL_UNMON_URLS):\n",
    "    sample = x[i]\n",
    "    timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "    packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "    cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "    cumulative_sum = 0\n",
    "    for j, c in enumerate(sample):\n",
    "        dr = 1 if c > 0 else -1\n",
    "        timestamps[j] = abs(c)\n",
    "        packet_sizes[j] = dr * 512\n",
    "        cumulative_sum += packet_sizes[j]\n",
    "        cum_sizes[j] = cumulative_sum\n",
    "\n",
    "    X_timestamps.append(timestamps)\n",
    "    X_packet_sizes.append(packet_sizes)\n",
    "    X_cum_sizes.append(cum_sizes)\n",
    "    y_monitored.append(-1)  # Label unmonitored data as -1\n",
    "\n",
    "print(f\"Total combined samples: {len(y_monitored)}\")\n",
    "\n",
    "# padding\n",
    "max_length = max(len(seq) for seq in X_timestamps)\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq[:maxlen]\n",
    "    return padded_sequences\n",
    "\n",
    "X_timestamps_padded = pad_sequences(X_timestamps, max_length)\n",
    "X_packet_sizes_padded = pad_sequences(X_packet_sizes, max_length)\n",
    "X_cum_sizes_padded = pad_sequences(X_cum_sizes, max_length)\n",
    "\n",
    "# feature combination\n",
    "X_combined = np.hstack((X_timestamps_padded, X_packet_sizes_padded, X_cum_sizes_padded))\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_monitored, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binary Classification (Monitored vs Unmonitored)\n",
    "binary_y_train = np.where(np.array(y_train) == -1, -1, 1)  # Monitored(1), Unmonitored(-1)\n",
    "binary_y_test = np.where(np.array(y_test) == -1, -1, 1)\n",
    "\n",
    "# Random Forest Binary Classification\n",
    "print(\"Training Open-World Binary Random Forest model...\")\n",
    "rf_binary_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_binary_model.fit(X_train, binary_y_train)\n",
    "\n",
    "binary_y_pred = rf_binary_model.predict(X_test)\n",
    "binary_accuracy = accuracy_score(binary_y_test, binary_y_pred)\n",
    "\n",
    "print(f\"Binary Classification Accuracy (Random Forest): {binary_accuracy:.4f}\")\n",
    "print(\"\\nBinary Classification Report (Random Forest):\")\n",
    "print(classification_report(binary_y_test, binary_y_pred))\n",
    "\n",
    "# Multi-Class Classification (Monitored and Unmonitored)\n",
    "print(\"Training Open-World Multi-Class Random Forest model...\")\n",
    "rf_multi_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_multi_model.fit(X_train, y_train)\n",
    "\n",
    "multi_y_pred = rf_multi_model.predict(X_test)\n",
    "multi_accuracy = accuracy_score(y_test, multi_y_pred)\n",
    "\n",
    "print(f\"Multi-Class Classification Accuracy (Random Forest): {multi_accuracy:.4f}\")\n",
    "print(\"\\nMulti-Class Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, multi_y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
