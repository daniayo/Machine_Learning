{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3N2W9dXUbdz"
   },
   "source": [
    "## Closed world : Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVCieo0mU3tk",
    "outputId": "24d6f49b-83c4-47c7-f067-9226c852ec6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading monitored datafile...\n",
      "\n",
      "Closed-world Multi-class Classification:\n",
      "Accuracy: 31.53%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.05      0.06        20\n",
      "           1       0.14      0.15      0.15        20\n",
      "           2       0.33      0.25      0.29        20\n",
      "           3       0.21      0.15      0.18        20\n",
      "           4       0.38      0.40      0.39        20\n",
      "           5       0.15      0.10      0.12        20\n",
      "           6       0.30      0.30      0.30        20\n",
      "           7       0.29      0.45      0.35        20\n",
      "           8       0.39      0.45      0.42        20\n",
      "           9       0.06      0.10      0.08        20\n",
      "          10       0.07      0.10      0.09        20\n",
      "          11       0.31      0.25      0.28        20\n",
      "          12       0.43      0.50      0.47        20\n",
      "          13       0.00      0.00      0.00        20\n",
      "          14       0.38      0.30      0.33        20\n",
      "          15       0.14      0.20      0.17        20\n",
      "          16       0.20      0.20      0.20        20\n",
      "          17       0.19      0.25      0.22        20\n",
      "          18       0.44      0.40      0.42        20\n",
      "          19       0.19      0.15      0.17        20\n",
      "          20       0.88      0.70      0.78        20\n",
      "          21       0.26      0.25      0.26        20\n",
      "          22       0.08      0.05      0.06        20\n",
      "          23       0.33      0.20      0.25        20\n",
      "          24       0.22      0.25      0.23        20\n",
      "          25       0.30      0.35      0.33        20\n",
      "          26       0.40      0.50      0.44        20\n",
      "          27       0.52      0.65      0.58        20\n",
      "          28       0.50      0.50      0.50        20\n",
      "          29       0.65      0.55      0.59        20\n",
      "          30       0.65      0.55      0.59        20\n",
      "          31       0.24      0.25      0.24        20\n",
      "          32       0.09      0.05      0.06        20\n",
      "          33       0.45      0.25      0.32        20\n",
      "          34       0.20      0.25      0.22        20\n",
      "          35       0.52      0.55      0.54        20\n",
      "          36       0.41      0.35      0.38        20\n",
      "          37       0.08      0.10      0.09        20\n",
      "          38       0.18      0.15      0.16        20\n",
      "          39       0.38      0.40      0.39        20\n",
      "          40       0.07      0.10      0.08        20\n",
      "          41       0.25      0.30      0.27        20\n",
      "          42       0.23      0.25      0.24        20\n",
      "          43       0.50      0.55      0.52        20\n",
      "          44       0.52      0.70      0.60        20\n",
      "          45       0.16      0.25      0.20        20\n",
      "          46       0.19      0.15      0.17        20\n",
      "          47       0.14      0.15      0.15        20\n",
      "          48       0.40      0.30      0.34        20\n",
      "          49       0.50      0.50      0.50        20\n",
      "          50       0.00      0.00      0.00        20\n",
      "          51       0.19      0.20      0.20        20\n",
      "          52       0.50      0.25      0.33        20\n",
      "          53       0.44      0.40      0.42        20\n",
      "          54       0.21      0.20      0.21        20\n",
      "          55       0.22      0.25      0.23        20\n",
      "          56       0.63      0.60      0.62        20\n",
      "          57       0.27      0.20      0.23        20\n",
      "          58       0.45      0.45      0.45        20\n",
      "          59       0.35      0.40      0.37        20\n",
      "          60       0.12      0.15      0.13        20\n",
      "          61       0.17      0.15      0.16        20\n",
      "          62       0.12      0.15      0.14        20\n",
      "          63       0.12      0.15      0.14        20\n",
      "          64       0.17      0.25      0.20        20\n",
      "          65       0.29      0.35      0.32        20\n",
      "          66       0.57      0.65      0.60        20\n",
      "          67       0.31      0.25      0.28        20\n",
      "          68       0.26      0.30      0.28        20\n",
      "          69       0.33      0.35      0.34        20\n",
      "          70       0.72      0.90      0.80        20\n",
      "          71       0.37      0.35      0.36        20\n",
      "          72       0.33      0.30      0.32        20\n",
      "          73       0.69      0.55      0.61        20\n",
      "          74       0.21      0.15      0.18        20\n",
      "          75       0.63      0.60      0.62        20\n",
      "          76       0.90      0.90      0.90        20\n",
      "          77       0.25      0.25      0.25        20\n",
      "          78       0.32      0.35      0.33        20\n",
      "          79       0.11      0.10      0.11        20\n",
      "          80       0.44      0.40      0.42        20\n",
      "          81       0.29      0.20      0.24        20\n",
      "          82       0.53      0.40      0.46        20\n",
      "          83       0.45      0.45      0.45        20\n",
      "          84       0.59      0.50      0.54        20\n",
      "          85       0.67      0.40      0.50        20\n",
      "          86       0.55      0.55      0.55        20\n",
      "          87       0.32      0.30      0.31        20\n",
      "          88       0.17      0.20      0.19        20\n",
      "          89       0.08      0.10      0.09        20\n",
      "          90       0.27      0.30      0.29        20\n",
      "          91       0.22      0.20      0.21        20\n",
      "          92       0.19      0.15      0.17        20\n",
      "          93       0.39      0.55      0.46        20\n",
      "          94       0.19      0.20      0.20        20\n",
      "\n",
      "    accuracy                           0.32      1900\n",
      "   macro avg       0.32      0.32      0.31      1900\n",
      "weighted avg       0.32      0.32      0.31      1900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"Loading monitored datafile...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", \"rb\") as f:\n",
    "    monitored_data = pickle.load(f)\n",
    "\n",
    "# Use top 1,000 samples\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS = 95\n",
    "MAX_SAMPLES = 1000\n",
    "\n",
    "all_samples = []\n",
    "all_labels = []\n",
    "\n",
    "for i, site_data in enumerate(monitored_data.values()):\n",
    "    label = i // URL_PER_SITE if not USE_SUBLABEL else i\n",
    "    for sample in site_data[:MAX_SAMPLES // TOTAL_URLS]:\n",
    "        all_samples.append(sample)\n",
    "        all_labels.append(label)\n",
    "\n",
    "#feature\n",
    "X_features = []\n",
    "for sample in all_samples:\n",
    "    # cumulative_sizes\n",
    "    cumulative_sizes = np.cumsum([val * 512 for val in sample])\n",
    "    # packet_sizes\n",
    "    packet_sizes = [val * 512 for val in sample]\n",
    "    # time stamp\n",
    "    transmission_times = [abs(val) for val in sample]\n",
    "\n",
    "    # feature combination\n",
    "    combined_features = np.concatenate(\n",
    "        [cumulative_sizes, packet_sizes, transmission_times]\n",
    "    )\n",
    "    X_features.append(combined_features)\n",
    "\n",
    "# padding\n",
    "max_length = max(len(sample) for sample in X_features)\n",
    "X_padded = pad_sequences(X_features, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# labeling\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_padded)\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# model training\n",
    "print(\"\\nClosed-world Multi-class Classification:\")\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# prediction and accuracy\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXIx92JkWqBm"
   },
   "source": [
    "## Closed world : SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zyE6koH_WyFT",
    "outputId": "2261ce12-9eba-42fa-8825-3892694b1190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datafile...\n",
      "Test set accuracy: 0.7255263157894737\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS   = 950\n",
    "MAX_LENGTH = 500\n",
    "\n",
    "\n",
    "print(\"Loading datafile...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", 'rb') as fi:\n",
    "    data = pickle.load(fi)\n",
    "\n",
    "def process_sample(sample):\n",
    "    size_seq = np.array([1 if c > 0 else -1 for c in sample], dtype=np.int16) * 512\n",
    "    time_seq = np.abs(sample).astype(np.float32)\n",
    "    cumulative_sizes = np.cumsum(size_seq)\n",
    "    return time_seq, size_seq, cumulative_sizes\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen))\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq[:maxlen]\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "\n",
    "X1 = [] # Array to store instances (timestamps) - 19,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
    "X2 = [] # Array to store instances (direction*size) - size information\n",
    "X3 = []\n",
    "y = [] # Array to store the site of each instance - 19,000 instances, e.g., [0, 0, 0, 0, 0, 0, ..., 94, 94, 94, 94, 94]\n",
    "\n",
    "# Differentiate instances and sites, and store them in the respective x and y arrays\n",
    "# x array (direction*timestamp), y array (site label)\n",
    "for i in range(TOTAL_URLS):\n",
    "    if USE_SUBLABEL:\n",
    "        label = i\n",
    "    else:\n",
    "        label = i // URL_PER_SITE\n",
    "\n",
    "    for sample in data[i]:\n",
    "        time_seq, size_seq, cumulative_sizes = process_sample(sample)\n",
    "        X1.append(time_seq)\n",
    "        X2.append(size_seq)\n",
    "        X3.append(cumulative_sizes)\n",
    "        y.append(label)\n",
    "\n",
    "X1_padded = pad_sequences(X1, MAX_LENGTH).astype(np.float32)\n",
    "X2_padded = pad_sequences(X2, MAX_LENGTH).astype(np.float32)\n",
    "X3_padded = pad_sequences(X3, MAX_LENGTH).astype(np.float32)\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# feature combination\n",
    "X = np.hstack((X1_padded, X2_padded,X3_padded))\n",
    "y = np.array(y)\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SVC(kernel=\"linear\",random_state=42, C=1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niximlWbXLDT"
   },
   "source": [
    "## Closed world : Tree ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4BJQ0APXRL3",
    "outputId": "dbec5632-6375-49de-fd7b-2a54a3471d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total monitored samples: 19000\n",
      "Training Closed-World Multi-Class Random Forest model...\n",
      "Multi-Class Classification Accuracy (Random Forest): 0.9079\n",
      "\n",
      "Multi-Class Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.86        40\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       1.00      0.95      0.97        40\n",
      "           3       0.90      0.95      0.93        40\n",
      "           4       0.95      0.97      0.96        40\n",
      "           5       0.94      0.82      0.88        40\n",
      "           6       0.93      0.93      0.93        40\n",
      "           7       0.84      0.95      0.89        40\n",
      "           8       0.95      0.90      0.92        40\n",
      "           9       0.90      0.88      0.89        40\n",
      "          10       0.94      0.82      0.88        40\n",
      "          11       0.97      0.95      0.96        40\n",
      "          12       0.93      0.95      0.94        40\n",
      "          13       0.86      0.95      0.90        40\n",
      "          14       0.97      0.90      0.94        40\n",
      "          15       0.71      0.90      0.79        40\n",
      "          16       0.84      0.80      0.82        40\n",
      "          17       0.92      0.90      0.91        40\n",
      "          18       0.93      0.95      0.94        40\n",
      "          19       0.85      0.85      0.85        40\n",
      "          20       0.95      1.00      0.98        40\n",
      "          21       0.95      1.00      0.98        40\n",
      "          22       0.90      0.90      0.90        40\n",
      "          23       0.89      1.00      0.94        40\n",
      "          24       0.79      0.78      0.78        40\n",
      "          25       0.88      0.72      0.79        40\n",
      "          26       0.67      0.93      0.78        40\n",
      "          27       0.93      0.95      0.94        40\n",
      "          28       0.95      0.90      0.92        40\n",
      "          29       0.92      0.90      0.91        40\n",
      "          30       0.86      0.95      0.90        40\n",
      "          31       0.93      0.93      0.93        40\n",
      "          32       0.83      0.88      0.85        40\n",
      "          33       0.97      0.93      0.95        40\n",
      "          34       1.00      0.80      0.89        40\n",
      "          35       0.95      0.93      0.94        40\n",
      "          36       0.95      0.90      0.92        40\n",
      "          37       0.90      0.88      0.89        40\n",
      "          38       0.86      0.80      0.83        40\n",
      "          39       0.87      0.85      0.86        40\n",
      "          40       0.88      0.90      0.89        40\n",
      "          41       0.86      0.95      0.90        40\n",
      "          42       0.92      0.85      0.88        40\n",
      "          43       0.82      0.93      0.87        40\n",
      "          44       0.92      0.90      0.91        40\n",
      "          45       0.89      0.82      0.86        40\n",
      "          46       0.97      0.97      0.97        40\n",
      "          47       0.88      0.93      0.90        40\n",
      "          48       0.93      0.95      0.94        40\n",
      "          49       0.90      0.95      0.93        40\n",
      "          50       0.94      0.80      0.86        40\n",
      "          51       0.93      0.93      0.93        40\n",
      "          52       0.95      0.88      0.91        40\n",
      "          53       0.86      0.90      0.88        40\n",
      "          54       0.92      0.88      0.90        40\n",
      "          55       0.97      0.88      0.92        40\n",
      "          56       1.00      0.88      0.93        40\n",
      "          57       0.86      0.90      0.88        40\n",
      "          58       0.83      1.00      0.91        40\n",
      "          59       0.97      0.90      0.94        40\n",
      "          60       0.88      0.88      0.88        40\n",
      "          61       0.85      0.82      0.84        40\n",
      "          62       0.95      0.90      0.92        40\n",
      "          63       0.92      0.88      0.90        40\n",
      "          64       0.97      0.93      0.95        40\n",
      "          65       0.87      0.85      0.86        40\n",
      "          66       1.00      1.00      1.00        40\n",
      "          67       0.95      0.90      0.92        40\n",
      "          68       0.83      0.85      0.84        40\n",
      "          69       0.97      0.95      0.96        40\n",
      "          70       0.89      0.97      0.93        40\n",
      "          71       0.90      0.93      0.91        40\n",
      "          72       1.00      0.93      0.96        40\n",
      "          73       0.93      0.95      0.94        40\n",
      "          74       0.93      0.93      0.93        40\n",
      "          75       0.93      1.00      0.96        40\n",
      "          76       0.95      0.90      0.92        40\n",
      "          77       0.88      0.90      0.89        40\n",
      "          78       0.97      0.90      0.94        40\n",
      "          79       0.92      0.90      0.91        40\n",
      "          80       1.00      0.93      0.96        40\n",
      "          81       0.95      0.90      0.92        40\n",
      "          82       0.90      0.88      0.89        40\n",
      "          83       0.86      0.93      0.89        40\n",
      "          84       1.00      0.97      0.99        40\n",
      "          85       0.93      0.95      0.94        40\n",
      "          86       0.80      1.00      0.89        40\n",
      "          87       0.93      0.93      0.93        40\n",
      "          88       0.92      0.90      0.91        40\n",
      "          89       0.84      0.90      0.87        40\n",
      "          90       1.00      0.97      0.99        40\n",
      "          91       0.89      0.97      0.93        40\n",
      "          92       0.86      0.90      0.88        40\n",
      "          93       0.95      0.97      0.96        40\n",
      "          94       0.84      0.80      0.82        40\n",
      "\n",
      "    accuracy                           0.91      3800\n",
      "   macro avg       0.91      0.91      0.91      3800\n",
      "weighted avg       0.91      0.91      0.91      3800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS = 950\n",
    "\n",
    "print(\"Loading data...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_timestamps = []  # Packet timestamps\n",
    "X_packet_sizes = []  # Packet sizes\n",
    "X_cum_sizes = []  # Cumulative packet size\n",
    "y_monitored = []  # Labels\n",
    "\n",
    "for i in range(TOTAL_URLS):\n",
    "    label = i if USE_SUBLABEL else i // URL_PER_SITE\n",
    "    for sample in data[i]:\n",
    "        timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "        packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "        cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "        cumulative_sum = 0\n",
    "        for j, c in enumerate(sample):\n",
    "            dr = 1 if c > 0 else -1\n",
    "            timestamps[j] = abs(c)\n",
    "            packet_sizes[j] = dr * 512\n",
    "            cumulative_sum += packet_sizes[j]\n",
    "            cum_sizes[j] = cumulative_sum\n",
    "\n",
    "        X_timestamps.append(timestamps)\n",
    "        X_packet_sizes.append(packet_sizes)\n",
    "        X_cum_sizes.append(cum_sizes)\n",
    "        y_monitored.append(label)\n",
    "\n",
    "print(f\"Total monitored samples: {len(y_monitored)}\")\n",
    "\n",
    "# padding\n",
    "max_length = max(len(seq) for seq in X_timestamps)\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq[:maxlen]\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "X_timestamps_padded = pad_sequences(X_timestamps, max_length)\n",
    "X_packet_sizes_padded = pad_sequences(X_packet_sizes, max_length)\n",
    "X_cum_sizes_padded = pad_sequences(X_cum_sizes, max_length)\n",
    "\n",
    "# feature combination\n",
    "X_combined = np.hstack((X_timestamps_padded, X_packet_sizes_padded, X_cum_sizes_padded))\n",
    "y_combined = np.array(y_monitored)\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined)\n",
    "\n",
    "# Random Forest Multi-Class Classification\n",
    "print(\"Training Closed-World Multi-Class Random Forest model...\")\n",
    "rf_multi_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_multi_model.fit(X_train, y_train)\n",
    "\n",
    "multi_y_pred_rf = rf_multi_model.predict(X_test)\n",
    "multi_accuracy_rf = accuracy_score(y_test, multi_y_pred_rf)\n",
    "\n",
    "print(f\"Multi-Class Classification Accuracy (Random Forest): {multi_accuracy_rf:.4f}\")\n",
    "print(\"\\nMulti-Class Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, multi_y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWWwWvfkZQy1"
   },
   "source": [
    "## Open world : Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MklnTMfZUIW",
    "outputId": "5460470b-2a42-4b56-e8d9-73774a960aa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total monitored samples: 19000\n",
      "Loading unmonitored datafile...\n",
      "Total combined samples: 22000\n",
      "Training Open-World Binary Decision Tree model...\n",
      "Binary Classification Accuracy (Decision Tree): 0.9998\n",
      "\n",
      "Binary Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       590\n",
      "           1       1.00      1.00      1.00      3810\n",
      "\n",
      "    accuracy                           1.00      4400\n",
      "   macro avg       1.00      1.00      1.00      4400\n",
      "weighted avg       1.00      1.00      1.00      4400\n",
      "\n",
      "Training Open-World Multi-Class Decision Tree model...\n",
      "Multi-Class Classification Accuracy (Decision Tree): 0.8473\n",
      "\n",
      "Multi-Class Classification Report (Decision Tree):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       590\n",
      "           0       0.84      0.70      0.76        30\n",
      "           1       0.91      0.89      0.90        44\n",
      "           2       0.83      0.83      0.83        41\n",
      "           3       0.90      0.85      0.88        33\n",
      "           4       0.76      0.81      0.79        32\n",
      "           5       0.82      0.89      0.86        37\n",
      "           6       0.84      0.95      0.89        38\n",
      "           7       0.72      0.74      0.73        35\n",
      "           8       0.73      0.73      0.73        33\n",
      "           9       0.41      0.62      0.49        26\n",
      "          10       0.92      0.79      0.85        43\n",
      "          11       0.85      0.87      0.86        45\n",
      "          12       0.97      0.84      0.90        44\n",
      "          13       0.67      0.70      0.68        46\n",
      "          14       0.88      1.00      0.94        37\n",
      "          15       0.73      0.77      0.75        39\n",
      "          16       0.84      0.73      0.78        51\n",
      "          17       0.85      0.74      0.79        53\n",
      "          18       0.88      1.00      0.94        38\n",
      "          19       0.70      0.82      0.76        38\n",
      "          20       0.96      0.96      0.96        48\n",
      "          21       0.97      1.00      0.99        34\n",
      "          22       0.76      0.61      0.67        51\n",
      "          23       0.78      0.79      0.78        39\n",
      "          24       0.59      0.53      0.56        38\n",
      "          25       0.82      0.74      0.78        38\n",
      "          26       0.94      0.84      0.89        37\n",
      "          27       0.89      0.89      0.89        47\n",
      "          28       0.67      0.84      0.74        31\n",
      "          29       0.93      0.86      0.89        43\n",
      "          30       0.81      0.90      0.85        39\n",
      "          31       0.84      0.82      0.83        44\n",
      "          32       0.69      0.71      0.70        48\n",
      "          33       0.95      0.90      0.92        39\n",
      "          34       0.76      0.69      0.72        36\n",
      "          35       0.82      0.84      0.83        32\n",
      "          36       0.86      0.88      0.87        42\n",
      "          37       0.68      0.71      0.70        42\n",
      "          38       0.63      0.59      0.61        49\n",
      "          39       0.86      0.72      0.78        43\n",
      "          40       0.69      0.76      0.72        45\n",
      "          41       0.81      0.83      0.82        46\n",
      "          42       0.90      0.88      0.89        43\n",
      "          43       0.89      0.84      0.86        49\n",
      "          44       0.89      0.98      0.93        42\n",
      "          45       0.68      0.67      0.67        45\n",
      "          46       0.93      0.87      0.90        31\n",
      "          47       0.89      0.81      0.85        48\n",
      "          48       0.98      0.93      0.95        44\n",
      "          49       0.88      0.82      0.85        34\n",
      "          50       0.88      0.91      0.89        32\n",
      "          51       0.98      0.94      0.96        48\n",
      "          52       0.89      0.72      0.80        57\n",
      "          53       0.80      0.88      0.84        41\n",
      "          54       0.71      0.89      0.79        36\n",
      "          55       0.86      0.84      0.85        44\n",
      "          56       0.97      0.95      0.96        41\n",
      "          57       0.97      0.91      0.94        32\n",
      "          58       0.90      0.90      0.90        42\n",
      "          59       0.94      0.89      0.91        36\n",
      "          60       0.63      0.77      0.69        35\n",
      "          61       0.76      0.78      0.77        37\n",
      "          62       0.77      0.79      0.78        43\n",
      "          63       0.88      0.86      0.87        43\n",
      "          64       0.84      0.89      0.86        35\n",
      "          65       0.74      0.82      0.78        39\n",
      "          66       0.93      0.77      0.84        35\n",
      "          67       0.76      0.90      0.83        42\n",
      "          68       0.75      0.82      0.79        40\n",
      "          69       0.90      0.90      0.90        42\n",
      "          70       0.91      1.00      0.95        41\n",
      "          71       0.81      0.90      0.85        29\n",
      "          72       0.89      0.89      0.89        56\n",
      "          73       0.93      0.93      0.93        43\n",
      "          74       0.77      0.75      0.76        32\n",
      "          75       0.89      1.00      0.94        40\n",
      "          76       0.85      0.92      0.88        37\n",
      "          77       0.71      0.71      0.71        31\n",
      "          78       0.93      0.82      0.87        33\n",
      "          79       0.68      0.75      0.71        53\n",
      "          80       1.00      0.93      0.96        40\n",
      "          81       0.94      0.74      0.83        39\n",
      "          82       0.70      0.75      0.72        40\n",
      "          83       0.75      0.69      0.72        39\n",
      "          84       0.85      0.78      0.81        36\n",
      "          85       1.00      0.85      0.92        47\n",
      "          86       0.80      0.69      0.74        35\n",
      "          87       0.76      0.86      0.81        37\n",
      "          88       0.62      0.78      0.69        27\n",
      "          89       0.74      0.70      0.72        40\n",
      "          90       0.92      0.88      0.90        51\n",
      "          91       0.84      0.80      0.82        40\n",
      "          92       0.70      0.79      0.74        29\n",
      "          93       0.94      0.92      0.93        50\n",
      "          94       0.81      0.83      0.82        35\n",
      "\n",
      "    accuracy                           0.85      4400\n",
      "   macro avg       0.83      0.83      0.82      4400\n",
      "weighted avg       0.85      0.85      0.85      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS = 950\n",
    "\n",
    "print(\"Loading data...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_timestamps = []  # Packet timestamps\n",
    "X_packet_sizes = []  # Packet sizes\n",
    "X_cum_sizes = []  # Cumulative packet size\n",
    "y_monitored = []  # Labels\n",
    "\n",
    "for i in range(TOTAL_URLS):\n",
    "    label = i if USE_SUBLABEL else i // URL_PER_SITE\n",
    "    for sample in data[i]:\n",
    "        timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "        packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "        cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "        cumulative_sum = 0\n",
    "        for j, c in enumerate(sample):\n",
    "            dr = 1 if c > 0 else -1\n",
    "            timestamps[j] = abs(c)\n",
    "            packet_sizes[j] = dr * 512\n",
    "            cumulative_sum += packet_sizes[j]\n",
    "            cum_sizes[j] = cumulative_sum\n",
    "\n",
    "        X_timestamps.append(timestamps)\n",
    "        X_packet_sizes.append(packet_sizes)\n",
    "        X_cum_sizes.append(cum_sizes)\n",
    "        y_monitored.append(label)\n",
    "\n",
    "print(f\"Total monitored samples: {len(y_monitored)}\")\n",
    "\n",
    "TOTAL_UNMON_URLS = 3000\n",
    "\n",
    "print(\"Loading unmonitored datafile...\")\n",
    "with open(\"/content/sample_data/unmon_standard10_3000.pkl\", \"rb\") as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "for i in range(TOTAL_UNMON_URLS):\n",
    "    sample = x[i]\n",
    "    timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "    packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "    cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "    cumulative_sum = 0\n",
    "    for j, c in enumerate(sample):\n",
    "        dr = 1 if c > 0 else -1\n",
    "        timestamps[j] = abs(c)\n",
    "        packet_sizes[j] = dr * 512\n",
    "        cumulative_sum += packet_sizes[j]\n",
    "        cum_sizes[j] = cumulative_sum\n",
    "\n",
    "    X_timestamps.append(timestamps)\n",
    "    X_packet_sizes.append(packet_sizes)\n",
    "    X_cum_sizes.append(cum_sizes)\n",
    "    y_monitored.append(-1)  # Label unmonitored data as -1\n",
    "\n",
    "print(f\"Total combined samples: {len(y_monitored)}\")\n",
    "\n",
    "# padding\n",
    "max_length = max(len(seq) for seq in X_timestamps)\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq[:maxlen]\n",
    "    return padded_sequences\n",
    "\n",
    "X_timestamps_padded = pad_sequences(X_timestamps, max_length)\n",
    "X_packet_sizes_padded = pad_sequences(X_packet_sizes, max_length)\n",
    "X_cum_sizes_padded = pad_sequences(X_cum_sizes, max_length)\n",
    "\n",
    "# feature combination\n",
    "X_combined = np.hstack((X_timestamps_padded, X_packet_sizes_padded, X_cum_sizes_padded))\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_monitored, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binary Classification (Monitored vs Unmonitored)\n",
    "binary_y_train = np.where(np.array(y_train) == -1, -1, 1)  # Monitored(1), Unmonitored(-1)\n",
    "binary_y_test = np.where(np.array(y_test) == -1, -1, 1)\n",
    "\n",
    "# Decision Tree Binary Classification\n",
    "print(\"Training Open-World Binary Decision Tree model...\")\n",
    "dt_binary_model = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "dt_binary_model.fit(X_train, binary_y_train)\n",
    "\n",
    "binary_y_pred_dt = dt_binary_model.predict(X_test)\n",
    "binary_accuracy_dt = accuracy_score(binary_y_test, binary_y_pred_dt)\n",
    "\n",
    "print(f\"Binary Classification Accuracy (Decision Tree): {binary_accuracy_dt:.4f}\")\n",
    "print(\"\\nBinary Classification Report (Decision Tree):\")\n",
    "print(classification_report(binary_y_test, binary_y_pred_dt))\n",
    "\n",
    "# Multi-Class Classification (Monitored and Unmonitored)\n",
    "print(\"Training Open-World Multi-Class Decision Tree model...\")\n",
    "dt_multi_model = DecisionTreeClassifier(random_state=42, class_weight=\"balanced\")\n",
    "dt_multi_model.fit(X_train, y_train)\n",
    "\n",
    "multi_y_pred_dt = dt_multi_model.predict(X_test)\n",
    "multi_accuracy_dt = accuracy_score(y_test, multi_y_pred_dt)\n",
    "\n",
    "print(f\"Multi-Class Classification Accuracy (Decision Tree): {multi_accuracy_dt:.4f}\")\n",
    "print(\"\\nMulti-Class Classification Report (Decision Tree):\")\n",
    "print(classification_report(y_test, multi_y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocKEEH0vZWwI"
   },
   "source": [
    "## Open world : Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVVRaBtdZb4a",
    "outputId": "ed000a4e-1b53-42f9-e426-505c43f505d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading monitored datafile...\n",
      "Total monitored samples: 19000\n",
      "Loading unmonitored datafile...\n",
      "Total combined samples: 22000\n",
      "Training Open-World Binary Random Forest model...\n",
      "Binary Classification Accuracy (Random Forest): 0.9932\n",
      "\n",
      "Binary Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.95      0.97       590\n",
      "           1       0.99      1.00      1.00      3810\n",
      "\n",
      "    accuracy                           0.99      4400\n",
      "   macro avg       1.00      0.98      0.99      4400\n",
      "weighted avg       0.99      0.99      0.99      4400\n",
      "\n",
      "Training Open-World Multi-Class Random Forest model...\n",
      "Multi-Class Classification Accuracy (Random Forest): 0.8925\n",
      "\n",
      "Multi-Class Classification Report (Random Forest):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.95      0.83       590\n",
      "           0       0.96      0.90      0.93        30\n",
      "           1       0.89      0.75      0.81        44\n",
      "           2       0.97      0.88      0.92        41\n",
      "           3       0.91      0.88      0.89        33\n",
      "           4       0.94      0.97      0.95        32\n",
      "           5       0.87      0.92      0.89        37\n",
      "           6       0.88      0.95      0.91        38\n",
      "           7       0.80      1.00      0.89        35\n",
      "           8       0.97      0.85      0.90        33\n",
      "           9       0.89      0.92      0.91        26\n",
      "          10       0.95      0.81      0.88        43\n",
      "          11       1.00      0.89      0.94        45\n",
      "          12       0.87      0.89      0.88        44\n",
      "          13       0.85      0.76      0.80        46\n",
      "          14       0.97      0.81      0.88        37\n",
      "          15       0.73      0.97      0.84        39\n",
      "          16       0.95      0.78      0.86        51\n",
      "          17       1.00      0.89      0.94        53\n",
      "          18       1.00      0.95      0.97        38\n",
      "          19       0.97      0.87      0.92        38\n",
      "          20       0.96      0.98      0.97        48\n",
      "          21       0.94      0.88      0.91        34\n",
      "          22       0.86      0.94      0.90        51\n",
      "          23       0.92      0.90      0.91        39\n",
      "          24       0.82      0.61      0.70        38\n",
      "          25       0.89      0.84      0.86        38\n",
      "          26       0.75      0.89      0.81        37\n",
      "          27       0.98      0.89      0.93        47\n",
      "          28       0.91      0.97      0.94        31\n",
      "          29       0.76      0.88      0.82        43\n",
      "          30       0.94      0.82      0.88        39\n",
      "          31       0.91      0.98      0.95        44\n",
      "          32       0.93      0.85      0.89        48\n",
      "          33       1.00      0.90      0.95        39\n",
      "          34       0.91      0.89      0.90        36\n",
      "          35       1.00      0.91      0.95        32\n",
      "          36       0.98      0.98      0.98        42\n",
      "          37       1.00      0.79      0.88        42\n",
      "          38       0.93      0.84      0.88        49\n",
      "          39       0.95      0.88      0.92        43\n",
      "          40       0.93      0.89      0.91        45\n",
      "          41       0.89      0.89      0.89        46\n",
      "          42       0.93      0.86      0.89        43\n",
      "          43       0.96      0.94      0.95        49\n",
      "          44       0.98      1.00      0.99        42\n",
      "          45       0.93      0.89      0.91        45\n",
      "          46       1.00      1.00      1.00        31\n",
      "          47       0.93      0.83      0.88        48\n",
      "          48       0.95      0.91      0.93        44\n",
      "          49       0.90      0.82      0.86        34\n",
      "          50       0.87      0.81      0.84        32\n",
      "          51       0.90      0.90      0.90        48\n",
      "          52       0.94      0.81      0.87        57\n",
      "          53       0.95      0.93      0.94        41\n",
      "          54       0.91      0.89      0.90        36\n",
      "          55       0.98      0.93      0.95        44\n",
      "          56       0.97      0.93      0.95        41\n",
      "          57       0.86      1.00      0.93        32\n",
      "          58       0.97      0.93      0.95        42\n",
      "          59       0.94      0.94      0.94        36\n",
      "          60       0.85      0.97      0.91        35\n",
      "          61       0.94      0.89      0.92        37\n",
      "          62       0.93      0.91      0.92        43\n",
      "          63       0.83      0.81      0.82        43\n",
      "          64       1.00      0.89      0.94        35\n",
      "          65       0.84      0.79      0.82        39\n",
      "          66       0.94      0.83      0.88        35\n",
      "          67       1.00      0.95      0.98        42\n",
      "          68       0.97      0.85      0.91        40\n",
      "          69       0.97      0.86      0.91        42\n",
      "          70       0.95      0.98      0.96        41\n",
      "          71       1.00      0.97      0.98        29\n",
      "          72       1.00      0.91      0.95        56\n",
      "          73       0.98      0.93      0.95        43\n",
      "          74       0.94      0.97      0.95        32\n",
      "          75       0.95      0.97      0.96        40\n",
      "          76       0.97      0.86      0.91        37\n",
      "          77       0.93      0.87      0.90        31\n",
      "          78       0.85      0.85      0.85        33\n",
      "          79       0.93      0.77      0.85        53\n",
      "          80       0.97      0.95      0.96        40\n",
      "          81       0.97      0.74      0.84        39\n",
      "          82       0.82      0.70      0.76        40\n",
      "          83       0.89      0.85      0.87        39\n",
      "          84       1.00      0.81      0.89        36\n",
      "          85       0.98      0.94      0.96        47\n",
      "          86       0.81      0.97      0.88        35\n",
      "          87       0.97      0.92      0.94        37\n",
      "          88       0.93      0.93      0.93        27\n",
      "          89       0.88      0.90      0.89        40\n",
      "          90       0.96      0.92      0.94        51\n",
      "          91       0.92      0.82      0.87        40\n",
      "          92       0.92      0.76      0.83        29\n",
      "          93       0.91      0.84      0.88        50\n",
      "          94       0.97      0.91      0.94        35\n",
      "\n",
      "    accuracy                           0.89      4400\n",
      "   macro avg       0.93      0.89      0.90      4400\n",
      "weighted avg       0.90      0.89      0.89      4400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS = 950\n",
    "\n",
    "print(\"Loading monitored datafile...\")\n",
    "with open(\"/content/sample_data/mon_standard.pkl\", \"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "X_timestamps = []  # Packet timestamps\n",
    "X_packet_sizes = []  # Packet sizes\n",
    "X_cum_sizes = []  # Cumulative packet size\n",
    "y_monitored = []  # Labels\n",
    "\n",
    "for i in range(TOTAL_URLS):\n",
    "    label = i if USE_SUBLABEL else i // URL_PER_SITE\n",
    "    for sample in data[i]:\n",
    "        timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "        packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "        cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "        cumulative_sum = 0\n",
    "        for j, c in enumerate(sample):\n",
    "            dr = 1 if c > 0 else -1\n",
    "            timestamps[j] = abs(c)\n",
    "            packet_sizes[j] = dr * 512\n",
    "            cumulative_sum += packet_sizes[j]\n",
    "            cum_sizes[j] = cumulative_sum\n",
    "\n",
    "        X_timestamps.append(timestamps)\n",
    "        X_packet_sizes.append(packet_sizes)\n",
    "        X_cum_sizes.append(cum_sizes)\n",
    "        y_monitored.append(label)\n",
    "\n",
    "print(f\"Total monitored samples: {len(y_monitored)}\")\n",
    "\n",
    "TOTAL_UNMON_URLS = 3000\n",
    "\n",
    "print(\"Loading unmonitored datafile...\")\n",
    "with open(\"/content/sample_data/unmon_standard10_3000.pkl\", \"rb\") as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "for i in range(TOTAL_UNMON_URLS):\n",
    "    sample = x[i]\n",
    "    timestamps = np.empty(len(sample), dtype=np.float32)\n",
    "    packet_sizes = np.empty(len(sample), dtype=np.int16)\n",
    "    cum_sizes = np.empty(len(sample), dtype=np.int32)\n",
    "\n",
    "    cumulative_sum = 0\n",
    "    for j, c in enumerate(sample):\n",
    "        dr = 1 if c > 0 else -1\n",
    "        timestamps[j] = abs(c)\n",
    "        packet_sizes[j] = dr * 512\n",
    "        cumulative_sum += packet_sizes[j]\n",
    "        cum_sizes[j] = cumulative_sum\n",
    "\n",
    "    X_timestamps.append(timestamps)\n",
    "    X_packet_sizes.append(packet_sizes)\n",
    "    X_cum_sizes.append(cum_sizes)\n",
    "    y_monitored.append(-1)  # Label unmonitored data as -1\n",
    "\n",
    "print(f\"Total combined samples: {len(y_monitored)}\")\n",
    "\n",
    "# padding\n",
    "max_length = max(len(seq) for seq in X_timestamps)\n",
    "\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :len(seq)] = seq[:maxlen]\n",
    "    return padded_sequences\n",
    "\n",
    "X_timestamps_padded = pad_sequences(X_timestamps, max_length)\n",
    "X_packet_sizes_padded = pad_sequences(X_packet_sizes, max_length)\n",
    "X_cum_sizes_padded = pad_sequences(X_cum_sizes, max_length)\n",
    "\n",
    "# feature combination\n",
    "X_combined = np.hstack((X_timestamps_padded, X_packet_sizes_padded, X_cum_sizes_padded))\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_monitored, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binary Classification (Monitored vs Unmonitored)\n",
    "binary_y_train = np.where(np.array(y_train) == -1, -1, 1)  # Monitored(1), Unmonitored(-1)\n",
    "binary_y_test = np.where(np.array(y_test) == -1, -1, 1)\n",
    "\n",
    "# Random Forest Binary Classification\n",
    "print(\"Training Open-World Binary Random Forest model...\")\n",
    "rf_binary_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_binary_model.fit(X_train, binary_y_train)\n",
    "\n",
    "binary_y_pred = rf_binary_model.predict(X_test)\n",
    "binary_accuracy = accuracy_score(binary_y_test, binary_y_pred)\n",
    "\n",
    "print(f\"Binary Classification Accuracy (Random Forest): {binary_accuracy:.4f}\")\n",
    "print(\"\\nBinary Classification Report (Random Forest):\")\n",
    "print(classification_report(binary_y_test, binary_y_pred))\n",
    "\n",
    "# Multi-Class Classification (Monitored and Unmonitored)\n",
    "print(\"Training Open-World Multi-Class Random Forest model...\")\n",
    "rf_multi_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_multi_model.fit(X_train, y_train)\n",
    "\n",
    "multi_y_pred = rf_multi_model.predict(X_test)\n",
    "multi_accuracy = accuracy_score(y_test, multi_y_pred)\n",
    "\n",
    "print(f\"Multi-Class Classification Accuracy (Random Forest): {multi_accuracy:.4f}\")\n",
    "print(\"\\nMulti-Class Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, multi_y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
